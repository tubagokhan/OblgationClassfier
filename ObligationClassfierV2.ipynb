{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "mount_file_id": "1-vDaikbUScdNXdQNL7rgXlUz-0MKCe_h",
      "authorship_tag": "ABX9TyNM7+DzGjPX9k0wASWTAXB8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/OblgationClassfier/blob/main/ObligationClassfierV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDFSbCdFOpMr",
        "outputId": "61ce6c30-0f94-46d7-be3e-07b23ec9c87c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1-Z7rDtJLh0m6QljxtfXSV4QdXRYbuxSf\n",
        "#!gdown 1-1L2_l6q01aD7Zsw_XMeVQfKtwP031Y9\n",
        "!gdown 1vJeCleL-o3m0TCTiKWNOxx4Gx2ZQOHwJ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvO437CEsmGH",
        "outputId": "244ef519-d469-4693-e983-0d87520d9f2f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-Z7rDtJLh0m6QljxtfXSV4QdXRYbuxSf\n",
            "To: /content/extended_obligation_classification_dataset.csv\n",
            "100% 14.3M/14.3M [00:00<00:00, 178MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vJeCleL-o3m0TCTiKWNOxx4Gx2ZQOHwJ\n",
            "To: /content/annotated_obligation_classification_data_pure_regulations.csv\n",
            "100% 1.69M/1.69M [00:00<00:00, 208MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-GKSRmfsfWN",
        "outputId": "80258581-3c3f-460e-d8fb-9ec5298cfb7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 5812\n",
            "Validation set size: 726\n",
            "Test set size: 728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 91/91 [00:18<00:00,  4.90it/s, batch_loss=0.129]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.3208320517461378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.21308319146434465, Validation accuracy: 0.9297520661157025\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 91/91 [00:18<00:00,  4.96it/s, batch_loss=0.154]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.16275908118420904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.14991482704256973, Validation accuracy: 0.953168044077135\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 91/91 [00:18<00:00,  5.03it/s, batch_loss=0.0562]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.1116926891545018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.14776965665320554, Validation accuracy: 0.9462809917355371\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 91/91 [00:17<00:00,  5.12it/s, batch_loss=0.0425]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.07293538810623872\n",
            "Validation loss: 0.16200864881587526, Validation accuracy: 0.9504132231404959\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 91/91 [00:17<00:00,  5.14it/s, batch_loss=0.0534]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.04379971509615144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.21646877378225327, Validation accuracy: 0.9504132231404959\n",
            "Early stopping triggered.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_linear_schedule_with_warmup, DistilBertConfig\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('./annotated_obligation_classification_data_pure_regulations.csv')\n",
        "data2 = pd.read_csv('./extended_obligation_classification_dataset.csv')\n",
        "\n",
        "# Merge datasets and remove duplicates\n",
        "#data = pd.concat([data, data2]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Ensure labels are numeric\n",
        "data['label'] = data['label'].map({'obligation': 1, 'non-obligation': 0})\n",
        "\n",
        "# Shuffle the data\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "train_size = int(0.8 * len(data))\n",
        "val_size = int(0.1 * len(data))\n",
        "test_size = len(data) - train_size - val_size\n",
        "\n",
        "train_data, val_data, test_data = np.split(data.sample(frac=1, random_state=42), [train_size, train_size + val_size])\n",
        "\n",
        "# Print the lengths of the datasets to ensure they are not empty\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")\n",
        "\n",
        "# Preprocess data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "max_len = 128  # Increased sequence length for better context\n",
        "batch_size = 64  # Increased batch size\n",
        "\n",
        "train_dataset = TextDataset(train_data['text'].to_list(), train_data['label'].to_list(), tokenizer, max_len)\n",
        "val_dataset = TextDataset(val_data['text'].to_list(), val_data['label'].to_list(), tokenizer, max_len)\n",
        "test_dataset = TextDataset(test_data['text'].to_list(), test_data['label'].to_list(), tokenizer, max_len)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)\n",
        "\n",
        "# Load model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Modify DistilBERT configuration to include dropout\n",
        "config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=2, dropout=0.3, attention_dropout=0.3)\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 10  # Increased number of epochs\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, scheduler, scaler, patience=2):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    best_loss = np.inf\n",
        "    patience_counter = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "        epoch_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            progress_bar.set_postfix(batch_loss=loss.item())\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f'Average training loss: {avg_loss}')\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "        print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model = model.state_dict()  # Save the best model\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        total_loss += epoch_loss\n",
        "    model.load_state_dict(best_model)  # Load the best model\n",
        "    return total_loss / (epoch + 1)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    preds, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            logits = outputs.logits\n",
        "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader) if len(val_loader) > 0 else float('inf')\n",
        "    accuracy = accuracy_score(true_labels, preds) if len(preds) > 0 else 0\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training loop\n",
        "train(model, train_loader, criterion, optimizer, scheduler, scaler, patience=2)\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), '/content/drive/Othercomputers/MBZUAI/MBZUAI/ADGM-Project/MyRetrievals/obligation_classification_model.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a_S2IZYGfJdf"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}